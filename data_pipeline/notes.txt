Apache Airflow
https://airflow.apache.org/
defines data pipelines
DAG = a collection of nodes and edges that describe the order of operations for a data pipeline
write DAG in python to run on a schedule and/or from external triggers
comes with web-based UI and command line utilities 

programmatically author, schedule, and monitor workflows 
author workflows as DAG of tasks
the airflow scheduler executes your tasks on an array of workers while following the specified dependencies
when workflows are defined as code, they become more maintainable, versionable, testable, and collaborative 

5 runtime components
1. scheduler
responsible for tracking progress of DAGs and their tasks
orchestrates execution of jobs on a trigger or schedule
can use '@daily' or cron 
if you don't schedule the default is 1x per day
if you don't set an end date it will continue to run
2. work queue
holds the state of running DAGs and tasks
3. worker processes
actually execute the individual processes of the DAG
4. database
stores credentials, connections, history, and configuration aka metadata
don't store actual data related to the processes in this DB
airflow components interact with the DB using SQLAlchemy
5. web server
provides UI dashboard for users and maintainers 
built using Flask

airflow itself is not a data processing framework like Spark or Hadoop
in airflow you don't pass data in memory between steps in your DAG
use airflow to coordinate the movement of data between other data storage + processing tools

find tasks that have no dependencies so execute them first
place tasks in the queue for workers to pick up the tasks + execute
after completion the status is recorded by the scheduler 
the scheduler will only place work in the queue if all the jobs dependencies have been met

airflow comes with a lot of useful operators like S3toRedshift
operators = abstract building block that can be configured to perform some work
task = instantiated in a pipeline fully parameterized for execution

managing connections
can manage connections and configurations in DAG UI
connections can be accessed in code via hooks
hook = reusable connection to an external database or system

admin
connections
create
conn id aws_credentials
conn type amazon web services
login aws key
password aws secret key

context variables
https://airflow.apache.org/macros.html
https://blog.godatadriven.com/zen-of-python-and-apache-airflow

execution date is helpful for backfilling because you can record the date you intended the code to run
not necessarily the current date or the time of execution
