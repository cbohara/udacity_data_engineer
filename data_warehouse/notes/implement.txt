##############
Redshift
##############
column-oriented storage
best suited for storing OLAP workloads, summing over long history
under the hood redshift is postgres with extensions to allow for columnar storage

traditional RDBMS
a single query 
- will perform a full table scan
- always executed on a single CPU of a single machine
	- this is acceptable because OLTP queries are not compute-intensive 
	- usually just updates or inserting a few rows

MPP (massive parallel processing) databases
a table is partitioned across multiple machines
each partition can be processed by a separate CPU in parallel

###########
architecture
###########
one leader node
accepts all client communication 
coordinates compute nodes
optimizes query execution

multiple compute node
has its own CPU, memory, and disk (depending on EC2 instance type)
scale up - have a few powerful nodes
scale out - having many average nodes

compute node slice
each compute node is logically divided into slices
slice = unit of parallelism
a cluster with n slices can process n partitions of a table simultaneously

can choose compute optimize nodes or storage optimized nodes

###############
ETL
###############
what if you want to transfer from mysql to postgres?
traditional solution
mysql > SELECT > ETL server > INSERT/COPY > postgres

ETL server will have both mysql and postgres clients installed on it
you can run a select statement on the ETL server to get all the data 
write the intermediate data into csv files on the ETL server
then insert/copy the csv files into postgres

AWS solution
ETL server is an EC2 instance that acts a client to RDS and Redshift to issue COPY commands
the storage itself will reside in S3
AWS RDS (relational database service) > COPY > S3 > COPY > Redshift

Redshift + ETL in context
diverse number of sources
S3 un-, semi-, and structured data
RDS RDBMS
DynamoDB columnar storage

ETL processes
get data from various sources into S3
then can be loaded to Redshift

Once in redshift > generate pre-aggregated OLAP cubes
these cubes can reside in S3, RDS, Cassandra

Then BI tools will query these pre-aggegrated metrics directly
Avoid using raw data

########################
Redshift specific ETL
########################
Ingestion = COPY
inserting data row by row is very slow
transfer data from an S3 staging area to Redshift using the COPY command

if file is large in size, better to break the file up into partitions
each Redshift slice will act as a separate worker and will use ingest the split of a file in parallel, so the process will complete much faster
allows COPY command to ingest the file in parallel
identify files below together by
1. common prefix OR 
2. manifest file

always better to be working with compressed data
the S3 stage should be in the same AWS region as the Redshift cluster

1. common prefix
COPY redshift_table_name FROM 's3://bucket/split/part'
CREDENTIALS 'iam role for Redshift to access S3' 
gzip delimiter ';' region 'us-west-2';

2. manifest file = explicit 
COPY redshift_table_name FROM 's3://bucket/files.manifest'
CREDENTIALS 'iam role for Redshift to access S3' 
manifest;

automatic compression optimization
you can choose the optimal compression strategy PER COLUMN
however COPY command will automatically makes best-effort compression decisions

Export
Redshift is accessible using JDBC
Usually used to hook up to BI apps
Will need to open up TCP ports in the Redshift node security group in order for the BI tools to access

UNLOAD
UNLOAD ('select * from venue limit 10;')
to 's3://'
iam role 'permissions for redshift to write to S3'

########################
Infrastructure as Code (IAC)
########################
Cloud Formation = specify resourcdes to launch
all resources must be launched successfully together
easy to tear down all resources as well


