##############
the power of spark
##############
###########
hardware
###########

CPU = central processing unit
"brain"
every process is handled by the CPU
typical CPU can execute over 2.5 billion operations per second

CPU can store small amounts of data inside itself = registers
holds the data the CPU is currently working with

ex: script reads in 40 MB data file + analyzes
when you execute the script, the instructions are loaded into the CPU
CPU then instructs the computer to take the 40 MB from disk + store data in RAM
if you want to sum a column of data, the CPU will take 2 numbers at a time and sum them together
the accumulation of the sum needs to be stored somewhere while the CPU grabs the next number
the cumulative sum will be stored in the register
registers make computations more efficient 
registers avoid having to send data unnecessarily back + forth between RAM and CPU

2.5 Gigahertz CPU processes 2.5 billion operations per second

memory (RAM)
"scratch paper"
data gets temporarily stored before getting sent to CPU
ephemeral storage

it takes 250x longer to find and load a random byte from memory vs process that byte in CPU
ex: in the time it takes to load an hours worth of random tweets in memory, a CPU could process over a week of tweets

characterists of memory
- efficiently loads data for CPU use
- ephemeral 
- expensive 

rather than rely on an expensive super computer with lots of memory like everyone else in the late 90s
Google took a different route
leveraged long-term storage on cheap pre-used hardware
distributed clusters = now industry standard

storage (SSD)
"filing cabinet"
long-term data storage
when a program runs, the CPU will direct the memory to temporarily load data from long term storage

network
access to the outside world
moving data across the network is the most common bottleneck when working with big data

ex: processing an hour of tweets (4.3 GB)
if data is in memory = takes 30ms to process
if data is in SSD and needs to be loaded into memory first = takes 0.5s to process
if data needs to be pulled down from twitter API via internet = 30 seconds

usually takes 20x longer to process data when you need to pull it down from another machine first
distributed systems try to minimize shuffling data between nodes as much as possible

key ratios
fastest
CPU = 200x faster vs loading data randomly from memory
memory = 15x faster vs long term storage SDD
SSD = 20x faster vs network

##################
what is big data?
##################
starting definition = cannot process the data on a single machine
need a cluster of computers to process the data

my machine has 32GB of RAM vs 1TB of SSD

what happens when you try to run a script on a local machine using a data file that is larger than what your RAM can hold?
the script will not work because
- memory could not load data quickly from storage
- CPU could not load data quickly from memory

CPU directed SSD to load 8GB of data into RAM, leaving no other memory for other processes + OS
CPU processed 8GB quickly + passed the 10GB results back to RAM > written to output file on SSD
the next 8GB then loaded from SSD > RAM = 3000x slower to load data from SSD vs actual CPU processing
the computer is spending most of its time moving data in and out of memory
most CPU activity is from coordinating data IO > thrashing when system is overwhelmed by IO

##############
hadoop ecosystem
##############
distributed computing
each node has its own CPU + memory 
communicate with each other by passing messages

parallel computing
form of distributed computing
the distinction is multiple CPUs have access to shared memory

##########
is there anything wrong with a data warehouse that we would need something different?
##########
nope 
for many orgs dw is still the best way to go
dimensional modeling is still very relevant
still need clean, consistent and performant model that business users can easily use to gain insights and make decisions
 
##########
so why a data lake?
##########
abundance of unstructured data
unprecedented volumes
emergence of data science role
new types of data analysis like predictive analytics, recommender systems 
 
##########
can we have unstructured data in the data warehouse?
##########
might be possible in the ETL process
ex: might be able to distill some elements from json data and put in tabular format
however you may later decide you want to transform the data differently
deciding on a particular transform is a strong commitment without enough knowledge 
some data is very difficult to get into tabular format (deep json structures)
some text/pdgs can be stored as blobs in a db but totally useless unless processed to extract metrics

##########
rise of big data technologies
##########
HDFS made it possible to store pedabytes of data on commodity hardware
much lower cost per TB vs traditional MPP (massive parallel processing) databases
MapReduce, Pig, Hive, Impala and Spark make it possible to process this data at scale on the same hardware used for storage
possible to make data analysis without inserting into predefined schema
one can load a csv file + make a query without making a table 
schema-on-read is great for processing unstructured data

advanced analytics
data scientists are restricted if they only can use the data warehouse single rigid representation of data
ds needs freedom to explore the data in its raw form 

offload ETL pressure from data warehouse to leverage new big data tools running on hadoop clusters

##########
schema-on-read
##########
traditionally data in a db table was much easier to process vs plain text files
Spark + other tools makes it easy to work with a file without reading into a database
you can let Spark infer the schema or you can specify the schema to be expected when reading it in to a dataframe
then you can run queries on the dataframe using SQL as if it is in a table

https://www.techopedia.com/definition/30153/schema-on-read
traditional databases = schema-on-write
aka the data has to be applied to a plan/schema when it is going into the database

companies now want to be able to store data no matter how messy it is
with Hadoop you do not have to worry about the data being structured going in
you can make sense of the data when you read the data = schema-on-read

Spark has the ability to read/write files in
- text based formats 
- binary formats like avro (save space) and parquet (columnar)
- compression formations

read/write to a variety of formats
- local file system
- HDFS
- S3

read/write to a variety of databases
- SQL via JDBC
- NoSQL

all spark data exposed in a single abstraction = data frame

##########
data lake concepts
##########
all types of data welcome = high/low value, un- semi- structured
ELT not ETL
data is stored as is in its original format
transforms are done later when processing the data 
data is processed with schema-on-read 
no predefined star schema is there before transformation
massive parallelism + scalability come out of the box with all big data processing tools
Spark packages available that make it easy to perform advanced analytics

##########
data lake options on AWS
##########
Storage			Processing			AWS-managed solution		Vendor managed
HDFS			Spark				AWS EMR (HDFS + Spark)		EC2 + vendor solution (Databricks, Cloudera, etc)
S3				Spark				AWS EMR (Spark only)		EC2 + vendor solution
S3				Serverless			AWS Athena					Serverless

AWS EMR (HDFS + Spark)
EMR Cluster = storage nodes and processsing nodes
Ingest data from S3 > EMR HDFS > query in place via Spark > BI + analytics apps
Cluster is not supposed to be shut down (ie caching cluster)
Cost = running each EC2 node at all times

AWS EMR (S3 + Spark)
Ingest data INTO S3 = lake storage
Lake is not in HDFS
Load data from S3 into Spark dataframe in EMR 
EMR only responsible for processing
Querying/processing results saved back to S3
Cluster does not need to be 24 hour on
Cheaper to save on S3 vs always on EC2 machines data nodes HDFS

############
AWS Athena
############
all data is stored in S3
athena is a service that can load and process data on serverless lambda resources
pay by execution time, not by machine up-time

setup Glue crawler to infer schema on the underlying S3 files
named the database pagila
s3://udacity-cbohara/pagila/
you can run the crawler on a schedule if you expect the schema of your input to be dynamic

by default glue will create a different table based on the subdirectory
s3://udacity-cbohara/pagila/actor/
created an actor table 
from the
s3://udacity-cbohara/pagila/actor/actor.csv

###########
considerations 
###########
prone to be a chaotic data dump so need to document everything 
data governance = telling who has access to what = hard bc all is available
you can still setup data lake in Athena using dimensional models > business results
