##############
the power of spark
##############
###########
hardware
###########

CPU = central processing unit
"brain"
every process is handled by the CPU
typical CPU can execute over 2.5 billion operations per second

CPU can store small amounts of data inside itself = registers
holds the data the CPU is currently working with

ex: script reads in 40 MB data file + analyzes
when you execute the script, the instructions are loaded into the CPU
CPU then instructs the computer to take the 40 MB from disk + store data in RAM
if you want to sum a column of data, the CPU will take 2 numbers at a time and sum them together
the accumulation of the sum needs to be stored somewhere while the CPU grabs the next number
the cumulative sum will be stored in the register
registers make computations more efficient 
registers avoid having to send data unnecessarily back + forth between RAM and CPU

2.5 Gigahertz CPU processes 2.5 billion operations per second

memory (RAM)
"scratch paper"
data gets temporarily stored before getting sent to CPU
ephemeral storage

it takes 250x longer to find and load a random byte from memory vs process that byte in CPU
ex: in the time it takes to load an hours worth of random tweets in memory, a CPU could process over a week of tweets

characterists of memory
- efficiently loads data for CPU use
- ephemeral 
- expensive 

rather than rely on an expensive super computer with lots of memory like everyone else in the late 90s
Google took a different route
leveraged long-term storage on cheap pre-used hardware
distributed clusters = now industry standard

storage (SSD)
"filing cabinet"
long-term data storage
when a program runs, the CPU will direct the memory to temporarily load data from long term storage

network
access to the outside world
moving data across the network is the most common bottleneck when working with big data

ex: processing an hour of tweets (4.3 GB)
if data is in memory = takes 30ms to process
if data is in SSD and needs to be loaded into memory first = takes 0.5s to process
if data needs to be pulled down from twitter API via internet = 30 seconds

usually takes 20x longer to process data when you need to pull it down from another machine first
distributed systems try to minimize shuffling data between nodes as much as possible

key ratios
fastest
CPU = 200x faster vs loading data randomly from memory
memory = 15x faster vs long term storage SDD
SSD = 20x faster vs network

##################
what is big data?
##################
starting definition = cannot process the data on a single machine
need a cluster of computers to process the data

my machine has 32GB of RAM vs 1TB of SSD

what happens when you try to run a script on a local machine using a data file that is larger than what your RAM can hold?
the script will not work because
- memory could not load data quickly from storage
- CPU could not load data quickly from memory

CPU directed SSD to load 8GB of data into RAM, leaving no other memory for other processes + OS
CPU processed 8GB quickly + passed the 10GB results back to RAM > written to output file on SSD
the next 8GB then loaded from SSD > RAM = 3000x slower to load data from SSD vs actual CPU processing
the computer is spending most of its time moving data in and out of memory
most CPU activity is from coordinating data IO > thrashing when system is overwhelmed by IO

##############
hadoop ecosystem
##############
distributed computing
each node has its own CPU + memory 
communicate with each other by passing messages

parallel computing
form of distributed computing
the distinction is multiple CPUs have access to shared memory

HDFS = data storage
splits data into chunks 
stores chunks across a cluster of computers

Hadoop MapReduce = programming model
MAP
each map step reads a partition from HDFS
converts input into key, value pair (aka tuple)

SHUFFLE
shuffle the kv pairs across the cluster so all keys are on the same node

REDUCE
compute the final result
values for a given key are combined

YARN = resource manager 
schedules jobs across the cluster
keeps track of compute resources available 
then assigns those available resources to specific tasks

###########
spark
###########
each node is responsible for a set of operations
at the end we combine the partial results to get the final answer 
master node = responsible for orchestrating tasks
worker nodes = perform actual operations

local mode
everything happens on a single machine
useful for prototyping

spark shell 
directly interact with driver program
