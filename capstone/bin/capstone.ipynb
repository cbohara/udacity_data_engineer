{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "A United States government agency is looking to better understand their immigration data.  On an annual basis they gather a resource that documents each immigration case in a Statistical Analysis Software file.  They also maintain a csv file with basic US city demographic information.  These datasets are stored on premise.\n",
    "\n",
    "As their data engineer, I am tasked with building an ETL pipeline that extracts their data from on premise, cleans and transforms data into a set of tables represented in parquet files in S3 to be queried in Athena for their analytics team to continue finding insights on US immigration patterns. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The analytics team would like to better understand their immigration data.  They are particularly interested in the demographic information of immigrants and the towns that they are drawn to live in within the United States.  They want to better understand how factors like age and visa category affect where people move.  They also want to know if people are drawn to cities in which people of a similar race are a significant portion of the population.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "One of the datasets I am using is the Visitor Arrivals Program (I-94 Record) data.  This dataset provides international visitor arrival statistics about country of origin, type of visa, age groups, and state of residence.  More information is available on their website: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "\n",
    "The country codes associated with country of origin in the I-94 data are made available in the resource 'I94_SAS_Labels_Descriptions.SAS'  This resource was parsed to generate the country_code.csv file.\n",
    "\n",
    "The continent associated with the country is made available in the country_to_contient.csv file made available at this website:\n",
    "https://datahub.io/JohnSnowLabs/country-and-continent-codes-list\n",
    "\n",
    "The dataset on US city demographics is provided by OpenSoft.  More information is available on their website:\n",
    "https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.window import Window\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Spark connection\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visitor Arrivals Program (I-94 Record) data\n",
    "raw_immigration_df = spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- validres: double (nullable = true)\n",
      " |-- delete_days: double (nullable = true)\n",
      " |-- delete_mexl: double (nullable = true)\n",
      " |-- delete_dup: double (nullable = true)\n",
      " |-- delete_visa: double (nullable = true)\n",
      " |-- delete_recdup: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US city demographics\n",
    "demo_schema = StructType([\n",
    "\tStructField(\"city\", StringType()),\n",
    "\tStructField(\"state\", StringType()),\n",
    "\tStructField(\"median_age\", DoubleType()),\n",
    "\tStructField(\"male_population\", IntegerType()),\n",
    "\tStructField(\"female_population\", IntegerType()),\n",
    "\tStructField(\"total_population\", IntegerType()),\n",
    "\tStructField(\"number_of_veterans\", IntegerType()),\n",
    "\tStructField(\"foreign_born\", IntegerType()),\n",
    "\tStructField(\"average_household_size\", DoubleType()),\n",
    "\tStructField(\"state_code\", StringType()),\n",
    "\tStructField(\"race\", StringType()),\n",
    "\tStructField(\"count\", IntegerType()),\n",
    "])\n",
    "\n",
    "us_demo_df = spark.read.csv('us-cities-demographics.csv', sep=';', schema=demo_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_demo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country code data extracted from I94_SAS_Labels_Descriptions.SAS\n",
    "country_code_df = spark.read.csv('country_code.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_id: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_code_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of country to continent\n",
    "continent_df = spark.read.csv('country_to_continent.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "continent_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Cleaning Steps\n",
    "The Visitor Arrivals Program (I-94 Record) data has room for improvement.  These improvements are made the immigration_df dataframe.  Most of the fields in the raw_immigration_df that are represented as decimal data types should be represented as integers, so these values are cast to integers.  The arrival date and departure date are also difficult to parse because of the special SAS date format, so this will be converted to a datetime data type that is more familiar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select fields of interest and cast double to integers\n",
    "immigration_df = raw_immigration_df.selectExpr(\n",
    "    'cast(cicid as int) AS id',\n",
    "    'cast(i94yr as int) AS year',\n",
    "    'cast(i94mon as int) AS month',\n",
    "    'cast(arrdate as int) AS arrdate',\n",
    "    'cast(depdate as int) AS depdate',\n",
    "    'cast(i94res as int) AS country_of_origin_code',\n",
    "    'i94addr AS state', \n",
    "    'cast(i94bir as int) AS age',\n",
    "    'gender',\n",
    "    'cast(i94visa as int) AS visa_category_code'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert SAS date to date datatype\n",
    "def to_datetime(sas_date):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(sas_date))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "sas_to_datetime_udf = udf(lambda sas_date: to_datetime(sas_date), DateType())\n",
    "\n",
    "immigration_df = immigration_df.withColumn(\"arrival_date\", sas_to_datetime_udf(\"arrdate\"))\\\n",
    "    .withColumn(\"departure_date\", sas_to_datetime_udf(\"depdate\"))\\\n",
    "    .drop(\"arrdate\", \"depdate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "I have opted to approach the data models using denormalized tables.  The goal of the project is to optimize for querying.  To avoid having to join several tables, I have opted to keep as much relevant information as possible in the table that accounts for US immigration data.  I have also opted to map codes to their names for ease of querying.  For example, I have mapped the visa code to its category name (i.e. visa category type 1 represents business related visas).  I have also mapped the country of origin code to the name of the country itself.  I also included the mapping of country to continent to better understand the continent of origin of the immigrant.\n",
    "\n",
    "Because the nature of the analysis, it is also useful to add an additional field to the US demographic table to calculate the percent representation of each race in each city.\n",
    "\n",
    "By having as much information as possible in the immigration data, it will be easier to join with a single other table - the US demographic information table.  These two tables can be joined by state if the data analysts want to better understand where US immigrants are residing.   \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The spark job will perform a series of tranformations and then write the data out to S3.  Data validation checks will be performed at the end of the job and if these validations fail then the job will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visa_category(category_code):\n",
    "    if category_code == 1:\n",
    "        return \"business\"\n",
    "    elif category_code == 2:\n",
    "        return \"pleasure\"\n",
    "    elif category_code == 3:\n",
    "        return \"student\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "    \n",
    "visa_category_udf = udf(lambda category_code: visa_category(category_code), StringType())\n",
    "\n",
    "immigration_df = immigration_df.withColumn(\"visa_category\", visa_category_udf(\"visa_category_code\"))\\\n",
    "    .drop(\"visa_category_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the country of origin data set to map the country of origin code to its name\n",
    "immigration_df = immigration_df.join(country_code_df, immigration_df.country_of_origin_code == country_code_df.country_id, how='left')\\\n",
    "    .drop(\"country_of_origin_code\", \"country_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "continent_df = spark.read.csv('country_to_continent.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- visa_category: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df = immigration_df.join(continent_df, immigration_df.country == continent_df.country_name, how='left')\\\n",
    "    .drop(\"country_name\")\n",
    "immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_percent(race_count, total_population):\n",
    "    return race_count/total_population\n",
    "\n",
    "race_percent_udf = udf(race_percent, DoubleType())\n",
    "us_demo_df = us_demo_df.withColumn(\"race_percent\", race_percent_udf(\"count\", \"total_population\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- visa_category: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- race_percent: double (nullable = true)\n",
      "\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+--------------------+\n",
      "|            city|         state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|                race| count|        race_percent|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+--------------------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|  0.3143712937923675|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|  0.6271881575152998|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|0.056094484847770486|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437| 0.13945512235208182|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402| 0.27101268831164227|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343| 0.01131795619453738|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592| 0.14367338844614108|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|  0.3015605268736923|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583| 0.03037679932260796|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060| 0.10070291728885165|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|   0.076236119840771|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|0.013068300858998533|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|  0.0782938060866048|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|   0.167101332204998|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791| 0.02254362682873665|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|  0.6778220630953186|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|  0.6861803308310199|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253| 0.00489856874220549|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|  0.2239297021600218|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847| 0.44247163592571465|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_demo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the immigration data out to parquet files partitioned by year and month of arrival\n",
    "immigration_df.write.partitionBy(\"year\", \"month\").parquet(\"s3://udacity-cbohara/immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the US demographic data out to parquet files partitioned by 2 letter state abbreviation (ex: CA for California)\n",
    "us_demo_df.write.partitionBy(\"state_code\").parquet(\"s3://udacity-cbohara/demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "In order to ensure the data was loaded correctly I will check the content of the S3 output via Athena to ensure it contains the same line count as the data frame.  These tables will also be made available to the data analysts for general queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df_count = immigration_df.count()\n",
    "us_demo_df_count = us_demo_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Create Athena table\n",
    "CREATE EXTERNAL TABLE public.immigration_fact_table (\n",
    "id int,\n",
    "year int,\n",
    "month int,\n",
    "state string,\n",
    "age int,\n",
    "gender string,\n",
    "arrival_date date,\n",
    "departure_date date,\n",
    "visa_category string,\n",
    "country string,\n",
    "continent string)\n",
    "PARTITIONED BY ( \n",
    "year int,\n",
    "month int)\n",
    "LOCATION 's3://udacity-cbohara/immigration';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Check line count equals data frame content\n",
    "select count(*)\n",
    "from public.immigration_fact_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Create Athena table\n",
    "CREATE EXTERNAL TABLE public.us_demographic_dimension_table (\n",
    "city string,\n",
    "state string,\n",
    "median_age double,\n",
    "male_population int,\n",
    "female_population int,\n",
    "total_population int,\n",
    "number_of_veterans int,\n",
    "foreign_born int,\n",
    "average_household_size double,\n",
    "state_code string,\n",
    "race string,\n",
    "count int\n",
    "race_percent double\n",
    ")\n",
    "PARTITIONED BY ( \n",
    "state_code string)\n",
    "LOCATION 's3://udacity-cbohara/demo';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Check line count equals data frame content\n",
    "select count(*)\n",
    "from public.us_demographic_dimension_table;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Fact Table\n",
    "##### Source - i94_jun16_sub.sas7bdat\n",
    "* id - CIC ID\n",
    "* year - I94 form submission year\n",
    "* month - I94 form submission month\n",
    "* state - state of residence in the United States during visa stay\n",
    "* age - age of immigrant\n",
    "* gender - gender of immigrant\n",
    "* arrival_date - arrival date into the United States (if applicable)\n",
    "* departure_date - departure date from the United States (if applicable)\n",
    "* visa_category - type of visa (business, student, or pleasure)\n",
    "* country - country of origin \n",
    "* continent - continent of origin\n",
    "\n",
    "#### US Demographic Dimension Table\n",
    "##### Sources - us-cities-demographics.csv, country_code.csv, country_to_continent.csv\n",
    "* city - name of city\n",
    "* state - name of state\n",
    "* median_age - median age of residents in the city\n",
    "* male_population - total male population count in the city \n",
    "* female_population - total female population count in the city\n",
    "* total_population - total population count in the city\n",
    "* number_of_veterans - number of veterans in the city\n",
    "* foreign_born - number of foreign born residents \n",
    "* average_household_size - average household size\n",
    "* state_code - 2 letter abbreviation of the state (ex: CA for California)\n",
    "* race - name of racial category\n",
    "* count - count of individuals associated with race in previous column\n",
    "* race_percent - percentage of individuals who identify as a certain race within city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "#### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "The goal of the project was to create a data lake with data relevant to immigration to the United States. This would include records of immigration and demographics for the cities.  It would be valuable for data analysts to have the ability to query the data lake.  This would allow them to better understand US immigration patterns.  For example, which regions are popular to immigrate to, and for what reason?  What demographic factors influence the migration to a certain location?\n",
    "\n",
    "I chose to use Spark for the ease of ingesting, transforming, and writing out datasets.  While a cluster was not required for the data sets of this size, it would be very easy to scale up and allocate more resources to the Spark job by spinning up a AWS EMR cluster or transitioning the Spark job to leverage AWS Glue, a serverless Spark service.\n",
    "\n",
    "I chose to create a denormalized fact table for the immigration to the United States, as well as a denormalized table representing US demographics.  The reason why I chose a denormalized approach is the data sets have been made available specifically for the ability of data analysis.\n",
    "\n",
    "I chose to use AWS Athena as a query tool because it is a serverless service that makes it easy to query S3 data in place.  You do not have to worry about the cost and maintaince of a AWS Redshift cluster.  It's an ideal tool for simple denormalized data sets that are written to S3 and that require little joins. \n",
    "\n",
    "I chose to partition the immigration fact table by year and month to make it easier for users to identify when a I94 record was submitted.  I chose to partition the US demographic dimension table by state because it is a natural categorization for United States demographic data.  These files were written in parquet format, which is the preferred file format for AWS Athena because you are charged by the amount of data you scan per query.  If your data is a more compact parquet format and you only choose certain columns to query, you can reduce the cost per query compared to writing out to CSV files. \n",
    "\n",
    "#### Propose how often the data should be updated and why.\n",
    "The data is updated on an annual basis, making the need for automation very low priority.  It is a task that can be manually enabled once a year to ingest the new I94 data set.\n",
    "\n",
    "#### Write a description of how you would approach the problem differently under the following scenarios:\n",
    "##### The data was increased by 100x.\n",
    "If the data increased, I would transition the job to use an AWS EMR cluster rather than a local machine to handle the increase memory requirements.\n",
    "\n",
    "##### Pipelines need to be run at 7am\n",
    "Given how infrequently the data is updated, this would not be a realistic requirement for the project.\n",
    "\n",
    "##### The database needed to be accessed by 100+ people.\n",
    "The beauty of AWS Athena is you don't need to manage a data warehouse yourself.  That being said, the standard limit is 20 DML queries (ie SELECT queries) can be run at the same time.  If you are an AWS partner, you can request that these service limits be increased to enable the 100+ people to access the data at a given time.\n",
    "\n",
    "If the service limit would not be able to be increased,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
